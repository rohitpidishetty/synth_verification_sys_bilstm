import re
from nltk.stem import WordNetLemmatizer
from sentence_transformers import SentenceTransformer
import torch
from .nnModel import BiLSTM_Attention_Model
from utils.download import download_model



stopwords = [
    # Standard stopwords
    "a",
    "about",
    "above",
    "after",
    "again",
    "against",
    "all",
    "am",
    "an",
    "and",
    "any",
    "are",
    "arent",
    "as",
    "at",
    "be",
    "because",
    "been",
    "before",
    "being",
    "below",
    "between",
    "both",
    "but",
    "by",
    "cant",
    "cannot",
    "could",
    "couldnt",
    "did",
    "didnt",
    "do",
    "does",
    "doesnt",
    "doing",
    "dont",
    "down",
    "during",
    "each",
    "few",
    "for",
    "from",
    "further",
    "had",
    "hadnt",
    "has",
    "hasnt",
    "have",
    "havent",
    "having",
    "he",
    "hed",
    "hell",
    "hes",
    "her",
    "here",
    "heres",
    "hers",
    "herself",
    "him",
    "himself",
    "his",
    "how",
    "hows",
    "i",
    "id",
    "ill",
    "im",
    "ive",
    "if",
    "in",
    "into",
    "is",
    "isnt",
    "it",
    "its",
    "itself",
    "lets",
    "me",
    "more",
    "most",
    "mustnt",
    "my",
    "myself",
    "no",
    "nor",
    "not",
    "of",
    "off",
    "on",
    "once",
    "only",
    "or",
    "other",
    "ought",
    "our",
    "ours",
    "ourselves",
    "out",
    "over",
    "own",
    "same",
    "she",
    "shed",
    "shell",
    "shes",
    "should",
    "shouldnt",
    "so",
    "some",
    "such",
    "than",
    "that",
    "thats",
    "the",
    "their",
    "theirs",
    "them",
    "themselves",
    "then",
    "there",
    "theres",
    "these",
    "they",
    "theyd",
    "theyll",
    "theyre",
    "theyve",
    "this",
    "those",
    "through",
    "to",
    "too",
    "under",
    "until",
    "up",
    "very",
    "was",
    "wasnt",
    "we",
    "wed",
    "well",
    "were",
    "weve",
    "were",
    "werent",
    "what",
    "whats",
    "when",
    "whens",
    "where",
    "wheres",
    "which",
    "while",
    "who",
    "whos",
    "whom",
    "why",
    "whys",
    "with",
    "wont",
    "would",
    "wouldnt",
    "you",
    "youd",
    "youll",
    "youre",
    "youve",
    "your",
    "yours",
    "yourself",
    "yourselves",
    # Social media filler words
    "rt",
    "via",
    "lol",
    "lmao",
    "omg",
    "idk",
    "tbh",
    "btw",
    "pls",
    "plz",
    "u",
    "ur",
    "r",
    "imho",
    "irl",
    "smh",
    "fyi",
    "yea",
    "yeah",
    "yup",
    "nope",
    "okay",
    "ok",
    "k",
    # Noise words
    "breaking",
    "update",
    "alert",
    "exclusive",
    "viral",
    "share",
    "repost",
    "read",
    "watch",
    "click",
    "follow",
    "true",
    "false",
    "real",
    "fake",
    "hoax",
    "scam",
    # Very frequent low-signal verbs
    "say",
    "says",
    "said",
    "tell",
    "told",
    "think",
    "thought",
    "know",
    "known",
    "report",
    "reported",
    "claim",
    "claimed",
    "claims",
    "show",
    "shown",
    "shows",
    "make",
    "makes",
    "made",
    "see",
    "seen",
    "look",
    "looks",
    # Generic nouns that almost never contribute to classification
    "people",
    "person",
    "man",
    "woman",
    "guy",
    "guys",
    "thing",
    "stuff",
    "someone",
    "everyone",
    "anyone",
    # Misinformation bait
    "wow",
    "shocking",
    "unbelievable",
    "insane",
    "must",
    "watch",
    "truth",
    "facts",
    "omfg",
    "literally",
]

lem = WordNetLemmatizer()
model = SentenceTransformer("all-mpnet-base-v2")


download_model()


model_lstm = BiLSTM_Attention_Model(input_dim=768, hidden_dim=256, num_classes=2)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_lstm.load_state_dict(
    torch.load("./model/yaari-synth-auditor-model-v1.pth", map_location=device)
)
model_lstm.to(device)

model_lstm.eval()


def cleanse(word):
    buffer = word.split()
    stream = ""
    for token in buffer:
        clean = lem.lemmatize(re.sub(r"[^a-zA-Z0-9]", "", token).lower()).strip()
        if clean not in stopwords:
            stream += clean + " "
    return stream



def auditor(data):
  emb = model.encode(data)
  X_single = torch.tensor(emb, dtype=torch.float32).unsqueeze(0)  
  with torch.no_grad():
      outputs, _ = model_lstm(X_single.to(device))
      pred_class = torch.argmax(torch.softmax(outputs, dim=1), dim=1).item()
  return pred_class

